{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>ex wife threaten suicid recent left wife good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>weird get affect compliment come someon know i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>final almost never hear bad year ever swear fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>need helpjust help im cri hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>lost hello name adam struggl year afraid past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232069</th>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>like rock go get anyth go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232070</th>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>tell mani friend lone everyth depriv pre bough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232071</th>\n",
       "      <td>pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>pee probabl tast like salti tea someon drank p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232072</th>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>usual stuff find post sympathi piti know far w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232073</th>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>still beaten first boss hollow knight fought t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232074 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        class  \\\n",
       "0       Ex Wife Threatening SuicideRecently I left my ...      suicide   \n",
       "1       Am I weird I don't get affected by compliments...  non-suicide   \n",
       "2       Finally 2020 is almost over... So I can never ...  non-suicide   \n",
       "3               i need helpjust help me im crying so hard      suicide   \n",
       "4       I‚Äôm so lostHello, my name is Adam (16) and I‚Äôv...      suicide   \n",
       "...                                                   ...          ...   \n",
       "232069  If you don't like rock then your not going to ...  non-suicide   \n",
       "232070  You how you can tell i have so many friends an...  non-suicide   \n",
       "232071  pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...  non-suicide   \n",
       "232072  The usual stuff you find hereI'm not posting t...      suicide   \n",
       "232073  I still haven't beaten the first boss in Hollo...  non-suicide   \n",
       "\n",
       "                                               clean_text  \n",
       "0       ex wife threaten suicid recent left wife good ...  \n",
       "1       weird get affect compliment come someon know i...  \n",
       "2       final almost never hear bad year ever swear fu...  \n",
       "3                          need helpjust help im cri hard  \n",
       "4       lost hello name adam struggl year afraid past ...  \n",
       "...                                                   ...  \n",
       "232069                          like rock go get anyth go  \n",
       "232070  tell mani friend lone everyth depriv pre bough...  \n",
       "232071  pee probabl tast like salti tea someon drank p...  \n",
       "232072  usual stuff find post sympathi piti know far w...  \n",
       "232073  still beaten first boss hollow knight fought t...  \n",
       "\n",
       "[232074 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/clean_text.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['clean_text'].astype(str).to_numpy()\n",
    "label = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\84359\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\84359\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = '<PAD>'\n",
    "UNKNOWN = '<BRUH>'\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data = [tokenize(text) for text in data]\n",
    "data\n",
    "modified_data = data.copy()\n",
    "modified_data.append([UNKNOWN, UNKNOWN, UNKNOWN, UNKNOWN, PADDING, PADDING, PADDING, PADDING, PADDING, PADDING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<BRUH>': 1,\n",
       " 'ex': 2,\n",
       " 'wife': 3,\n",
       " 'threaten': 4,\n",
       " 'suicid': 5,\n",
       " 'recent': 6,\n",
       " 'left': 7,\n",
       " 'good': 8,\n",
       " 'cheat': 9,\n",
       " 'twice': 10,\n",
       " 'lie': 11,\n",
       " 'much': 12,\n",
       " 'decid': 13,\n",
       " 'refus': 14,\n",
       " 'go': 15,\n",
       " 'back': 16,\n",
       " 'day': 17,\n",
       " 'ago': 18,\n",
       " 'began': 19,\n",
       " 'tirelessli': 20,\n",
       " 'spent': 21,\n",
       " 'paat': 22,\n",
       " 'talk': 23,\n",
       " 'keep': 24,\n",
       " 'hesit': 25,\n",
       " 'want': 26,\n",
       " 'believ': 27,\n",
       " 'come': 28,\n",
       " 'know': 29,\n",
       " 'lot': 30,\n",
       " 'peopl': 31,\n",
       " 'order': 32,\n",
       " 'get': 33,\n",
       " 'way': 34,\n",
       " 'happen': 35,\n",
       " 'realli': 36,\n",
       " 'suppos': 37,\n",
       " 'handl': 38,\n",
       " 'death': 39,\n",
       " 'hand': 40,\n",
       " 'still': 41,\n",
       " 'love': 42,\n",
       " 'can': 43,\n",
       " 'not': 44,\n",
       " 'deal': 45,\n",
       " 'constantli': 46,\n",
       " 'feel': 47,\n",
       " 'insecur': 48,\n",
       " 'worri': 49,\n",
       " 'today': 50,\n",
       " 'may': 51,\n",
       " 'hope': 52,\n",
       " 'weird': 53,\n",
       " 'affect': 54,\n",
       " 'compliment': 55,\n",
       " 'someon': 56,\n",
       " 'irl': 57,\n",
       " 'internet': 58,\n",
       " 'stranger': 59,\n",
       " 'final': 60,\n",
       " 'almost': 61,\n",
       " 'never': 62,\n",
       " 'hear': 63,\n",
       " 'bad': 64,\n",
       " 'year': 65,\n",
       " 'ever': 66,\n",
       " 'swear': 67,\n",
       " 'fuck': 68,\n",
       " 'god': 69,\n",
       " 'annoy': 70,\n",
       " 'need': 71,\n",
       " 'helpjust': 72,\n",
       " 'help': 73,\n",
       " 'im': 74,\n",
       " 'cri': 75,\n",
       " 'hard': 76,\n",
       " 'lost': 77,\n",
       " 'hello': 78,\n",
       " 'name': 79,\n",
       " 'adam': 80,\n",
       " 'struggl': 81,\n",
       " 'afraid': 82,\n",
       " 'past': 83,\n",
       " 'thought': 84,\n",
       " 'fear': 85,\n",
       " 'anxieti': 86,\n",
       " 'close': 87,\n",
       " 'limit': 88,\n",
       " 'quiet': 89,\n",
       " 'long': 90,\n",
       " 'scare': 91,\n",
       " 'famili': 92,\n",
       " 'lose': 93,\n",
       " 'aunt': 94,\n",
       " 'trigger': 95,\n",
       " 'everyday': 96,\n",
       " 'hopeless': 97,\n",
       " 'guilti': 98,\n",
       " 'remors': 99,\n",
       " 'thing': 100,\n",
       " 'done': 101,\n",
       " 'life': 102,\n",
       " 'like': 103,\n",
       " 'littl': 104,\n",
       " 'experienc': 105,\n",
       " 'time': 106,\n",
       " 'reveal': 107,\n",
       " 'broke': 108,\n",
       " 'saw': 109,\n",
       " 'cut': 110,\n",
       " 'watch': 111,\n",
       " 'someth': 112,\n",
       " 'portray': 113,\n",
       " 'averag': 114,\n",
       " 'made': 115,\n",
       " 'absolut': 116,\n",
       " 'dread': 117,\n",
       " 'later': 118,\n",
       " 'found': 119,\n",
       " 'attempt': 120,\n",
       " 'survivor': 121,\n",
       " 'od': 122,\n",
       " 'overdos': 123,\n",
       " 'pill': 124,\n",
       " 'hang': 125,\n",
       " 'blackout': 126,\n",
       " 'went': 127,\n",
       " 'noos': 128,\n",
       " 'first': 129,\n",
       " 'therapi': 130,\n",
       " 'diagnos': 131,\n",
       " 'sever': 132,\n",
       " 'depress': 133,\n",
       " 'social': 134,\n",
       " 'eat': 135,\n",
       " 'disord': 136,\n",
       " 'transfer': 137,\n",
       " 'fucken': 138,\n",
       " 'group': 139,\n",
       " 'reason': 140,\n",
       " 'anxiou': 141,\n",
       " 'eventu': 142,\n",
       " 'last': 143,\n",
       " 'session': 144,\n",
       " 'show': 145,\n",
       " 'result': 146,\n",
       " 'daili': 147,\n",
       " 'check': 148,\n",
       " 'step': 149,\n",
       " 'survey': 150,\n",
       " 'mom': 151,\n",
       " 'dad': 152,\n",
       " 'find': 153,\n",
       " 'put': 154,\n",
       " 'horribl': 155,\n",
       " 'amaz': 156,\n",
       " 'describ': 157,\n",
       " 'happiest': 158,\n",
       " 'seen': 159,\n",
       " 'sertalin': 160,\n",
       " 'anti': 161,\n",
       " 'sorri': 162,\n",
       " 'forgot': 163,\n",
       " 'finish': 164,\n",
       " 'prescript': 165,\n",
       " 'right': 166,\n",
       " 'type': 167,\n",
       " 'drug': 168,\n",
       " 'took': 169,\n",
       " 'recommend': 170,\n",
       " 'schedul': 171,\n",
       " 'week': 172,\n",
       " 'stop': 173,\n",
       " 'take': 174,\n",
       " 'wors': 175,\n",
       " 'damag': 176,\n",
       " 'caus': 177,\n",
       " 'even': 178,\n",
       " 'everyth': 179,\n",
       " 'relaps': 180,\n",
       " 'develop': 181,\n",
       " 'insomnia': 182,\n",
       " 'worthless': 183,\n",
       " 'question': 184,\n",
       " 'motiv': 185,\n",
       " 'move': 186,\n",
       " 'bed': 187,\n",
       " 'ask': 188,\n",
       " 'nearli': 189,\n",
       " 'everi': 190,\n",
       " 'night': 191,\n",
       " 'break': 192,\n",
       " 'everytim': 193,\n",
       " 'pleas': 194,\n",
       " 'anyon': 195,\n",
       " 'might': 196,\n",
       " 'drastic': 197,\n",
       " 'shape': 198,\n",
       " 'idk': 199,\n",
       " 'anymor': 200,\n",
       " 'honetli': 201,\n",
       " 'dont': 202,\n",
       " 'noth': 203,\n",
       " 'nowher': 204,\n",
       " 'either': 205,\n",
       " 'unbear': 206,\n",
       " 'sad': 207,\n",
       " 'ignor': 208,\n",
       " 'friend': 209,\n",
       " 'opitun': 210,\n",
       " 'loos': 211,\n",
       " 'girlfriend': 212,\n",
       " 'hurt': 213,\n",
       " 'everyon': 214,\n",
       " 'anyth': 215,\n",
       " 'behind': 216,\n",
       " 'educ': 217,\n",
       " 'alon': 218,\n",
       " 'ive': 219,\n",
       " 'enjoy': 220,\n",
       " 'dream': 221,\n",
       " 'care': 222,\n",
       " 'complic': 223,\n",
       " 'word': 224,\n",
       " 'would': 225,\n",
       " 'end': 226,\n",
       " 'strong': 227,\n",
       " 'brave': 228,\n",
       " 'enough': 229,\n",
       " 'weak': 230,\n",
       " 'make': 231,\n",
       " 'sadder': 232,\n",
       " 'push': 233,\n",
       " 'away': 234,\n",
       " 'emot': 235,\n",
       " 'empti': 236,\n",
       " 'use': 237,\n",
       " 'normal': 238,\n",
       " 'understand': 239,\n",
       " 'mention': 240,\n",
       " 'got': 241,\n",
       " 'die': 242,\n",
       " 'havnt': 243,\n",
       " 'brought': 244,\n",
       " 'realis': 245,\n",
       " 'cant': 246,\n",
       " 'comprehend': 247,\n",
       " 'mean': 248,\n",
       " 'rambl': 249,\n",
       " 'probabl': 250,\n",
       " 'regret': 251,\n",
       " 'post': 252,\n",
       " 'ill': 253,\n",
       " 'think': 254,\n",
       " 'place': 255,\n",
       " 'gun': 256,\n",
       " 'head': 257,\n",
       " 'encoag': 258,\n",
       " 'see': 259,\n",
       " 'instead': 260,\n",
       " 'suviv': 261,\n",
       " 'plu': 262,\n",
       " 'meaningless': 263,\n",
       " 'futur': 264,\n",
       " 'bleak': 265,\n",
       " 'could': 266,\n",
       " 'cure': 267,\n",
       " 'cancer': 268,\n",
       " 'wast': 269,\n",
       " 'warn': 270,\n",
       " 'excus': 271,\n",
       " 'self': 272,\n",
       " 'inflict': 273,\n",
       " 'burn': 274,\n",
       " 'crisi': 275,\n",
       " 'line': 276,\n",
       " 'panic': 277,\n",
       " 'attack': 278,\n",
       " 'healthi': 279,\n",
       " 'stupid': 280,\n",
       " 'impuls': 281,\n",
       " 'father': 282,\n",
       " 'daughter': 283,\n",
       " 'histori': 284,\n",
       " 'togeth': 285,\n",
       " 'worst': 286,\n",
       " 'alway': 287,\n",
       " 'ankl': 288,\n",
       " 'wrist': 289,\n",
       " 'easier': 290,\n",
       " 'one': 291,\n",
       " 'work': 292,\n",
       " 'car': 293,\n",
       " 'harm': 294,\n",
       " 'without': 295,\n",
       " 'usual': 296,\n",
       " 'moment': 297,\n",
       " 'say': 298,\n",
       " 'touch': 299,\n",
       " 'hood': 300,\n",
       " 'hot': 301,\n",
       " 'curv': 302,\n",
       " 'pattern': 303,\n",
       " 'forearm': 304,\n",
       " 'side': 305,\n",
       " 'inch': 306,\n",
       " 'kind': 307,\n",
       " 'wide': 308,\n",
       " 'deep': 309,\n",
       " 'explain': 310,\n",
       " 'mayb': 311,\n",
       " 'wire': 312,\n",
       " 'smoosh': 313,\n",
       " 'engin': 314,\n",
       " 'fix': 315,\n",
       " 'abl': 316,\n",
       " 'tonight': 317,\n",
       " 'quit': 318,\n",
       " 'edgi': 319,\n",
       " 'consciou': 320,\n",
       " 'stand': 321,\n",
       " 'draw': 322,\n",
       " 'ye': 323,\n",
       " 'play': 324,\n",
       " 'guitar': 325,\n",
       " 'honestli': 326,\n",
       " 'stuck': 327,\n",
       " 'tast': 328,\n",
       " 'music': 329,\n",
       " 'rock': 330,\n",
       " 'alt': 331,\n",
       " 'metal': 332,\n",
       " 'uniqu': 333,\n",
       " 'style': 334,\n",
       " 'classmat': 335,\n",
       " 'rap': 336,\n",
       " 'edm': 337,\n",
       " 'fit': 338,\n",
       " 'other': 339,\n",
       " 'copi': 340,\n",
       " 'anoth': 341,\n",
       " 'quirki': 342,\n",
       " 'kid': 343,\n",
       " 'cringey': 344,\n",
       " 'phase': 345,\n",
       " 'mani': 346,\n",
       " 'look': 347,\n",
       " 'grung': 348,\n",
       " 'kinda': 349,\n",
       " 'agre': 350,\n",
       " 'continu': 351,\n",
       " 'wore': 352,\n",
       " 'cross': 353,\n",
       " 'wallet': 354,\n",
       " 'chain': 355,\n",
       " 'tiktok': 356,\n",
       " 'categori': 357,\n",
       " 'confus': 358,\n",
       " 'clout': 359,\n",
       " 'chaser': 360,\n",
       " 'boy': 361,\n",
       " 'goddamn': 362,\n",
       " 'hate': 363,\n",
       " 'old': 364,\n",
       " 'bald': 365,\n",
       " 'male': 366,\n",
       " 'hairlin': 367,\n",
       " 'trash': 368,\n",
       " 'matter': 369,\n",
       " 'huge': 370,\n",
       " 'bipolar': 371,\n",
       " 'crippl': 372,\n",
       " 'cherri': 373,\n",
       " 'top': 374,\n",
       " 'wear': 375,\n",
       " 'hat': 376,\n",
       " 'room': 377,\n",
       " 'pop': 378,\n",
       " 'xanax': 379,\n",
       " 'tri': 380,\n",
       " 'numb': 381,\n",
       " 'pain': 382,\n",
       " 'bit': 383,\n",
       " 'crash': 384,\n",
       " 'commun': 385,\n",
       " 'relationship': 386,\n",
       " 'popular': 387,\n",
       " 'pass': 388,\n",
       " 'dark': 389,\n",
       " 'hole': 390,\n",
       " 'arrest': 391,\n",
       " 'numer': 392,\n",
       " 'rehab': 393,\n",
       " 'mental': 394,\n",
       " 'hospit': 395,\n",
       " 'kill': 396,\n",
       " 'yet': 397,\n",
       " 'brother': 398,\n",
       " 'dead': 399,\n",
       " 'point': 400,\n",
       " 'support': 401,\n",
       " 'aliv': 402,\n",
       " 'guy': 403,\n",
       " 'child': 404,\n",
       " 'molestor': 405,\n",
       " 'choos': 406,\n",
       " 'rest': 407,\n",
       " 'sleep': 408,\n",
       " 'painkil': 409,\n",
       " 'wait': 410,\n",
       " 'imagin': 411,\n",
       " 'neither': 412,\n",
       " 'wrinkl': 413,\n",
       " 'weight': 414,\n",
       " 'gain': 415,\n",
       " 'hair': 416,\n",
       " 'loss': 417,\n",
       " 'mess': 418,\n",
       " 'teeth': 419,\n",
       " 'bone': 420,\n",
       " 'health': 421,\n",
       " 'issu': 422,\n",
       " 'menopaus': 423,\n",
       " 'hormon': 424,\n",
       " 'new': 425,\n",
       " 'gener': 426,\n",
       " 'amp': 427,\n",
       " 'world': 428,\n",
       " 'progress': 429,\n",
       " 'useless': 430,\n",
       " 'angri': 431,\n",
       " 'piec': 432,\n",
       " 'shit': 433,\n",
       " 'total': 434,\n",
       " 'depend': 435,\n",
       " 'secretli': 436,\n",
       " 'alreadi': 437,\n",
       " 'happi': 438,\n",
       " 'avoid': 439,\n",
       " 'hit': 440,\n",
       " 'train': 441,\n",
       " 'countri': 442,\n",
       " 'suffer': 443,\n",
       " 'though': 444,\n",
       " 'painless': 445,\n",
       " 'method': 446,\n",
       " 'interest': 447,\n",
       " 'inform': 448,\n",
       " 'bunch': 449,\n",
       " 'spit': 450,\n",
       " 'person': 451,\n",
       " 'obvious': 452,\n",
       " 'least': 453,\n",
       " 'troll': 454,\n",
       " 'laugh': 455,\n",
       " 'desir': 456,\n",
       " 'termin': 457,\n",
       " 'grow': 458,\n",
       " 'stronger': 459,\n",
       " 'bitter': 460,\n",
       " 'main': 461,\n",
       " 'goal': 462,\n",
       " 'throughout': 463,\n",
       " 'process': 464,\n",
       " 'minim': 465,\n",
       " 'subsequ': 466,\n",
       " 'fallout': 467,\n",
       " 'certainli': 468,\n",
       " 'nice': 469,\n",
       " 'patron': 470,\n",
       " 'forum': 471,\n",
       " 'respect': 472,\n",
       " 'privaci': 473,\n",
       " 'ridicul': 474,\n",
       " 'expect': 475,\n",
       " 'consid': 476,\n",
       " 'sourc': 477,\n",
       " 'edit': 478,\n",
       " 'verizon': 479,\n",
       " 'smart': 480,\n",
       " 'app': 481,\n",
       " 'porn': 482,\n",
       " 'privat': 483,\n",
       " 'wtf': 484,\n",
       " 'featur': 485,\n",
       " 'seem': 486,\n",
       " 'young': 487,\n",
       " 'transgend': 488,\n",
       " 'sure': 489,\n",
       " 'tell': 490,\n",
       " 'actual': 491,\n",
       " 'tran': 492,\n",
       " 'overwhelm': 493,\n",
       " 'wish': 494,\n",
       " 'religi': 495,\n",
       " 'accept': 496,\n",
       " 'allevi': 497,\n",
       " 'yesterday': 498,\n",
       " 'bare': 499,\n",
       " 'drew': 500,\n",
       " 'blood': 501,\n",
       " 'correctli': 502,\n",
       " 'pursu': 503,\n",
       " 'money': 504,\n",
       " 'field': 505,\n",
       " 'unless': 506,\n",
       " 'becom': 507,\n",
       " 'famou': 508,\n",
       " 'current': 509,\n",
       " 'serious': 510,\n",
       " 'debat': 511,\n",
       " 'longer': 512,\n",
       " 'born': 513,\n",
       " 'girl': 514,\n",
       " 'well': 515,\n",
       " 'screw': 516,\n",
       " 'lock': 517,\n",
       " 'school': 518,\n",
       " 'toilet': 519,\n",
       " 'live': 520,\n",
       " 'stori': 521,\n",
       " 'assign': 522,\n",
       " 'due': 523,\n",
       " 'tomorrow': 524,\n",
       " 'start': 525,\n",
       " 'yeaput': 526,\n",
       " 'knife': 527,\n",
       " 'give': 528,\n",
       " 'free': 529,\n",
       " 'fun': 530,\n",
       " 'sister': 531,\n",
       " 'goe': 532,\n",
       " 'realiz': 533,\n",
       " 'haha': 534,\n",
       " 'goodby': 535,\n",
       " 'disabl': 536,\n",
       " 'ptsd': 537,\n",
       " 'rheumatoid': 538,\n",
       " 'arthriti': 539,\n",
       " 'lb': 540,\n",
       " 'sick': 541,\n",
       " 'tire': 542,\n",
       " 'singl': 543,\n",
       " 'reject': 544,\n",
       " 'monster': 545,\n",
       " 'connect': 546,\n",
       " 'companionship': 547,\n",
       " 'loneli': 548,\n",
       " 'taken': 549,\n",
       " 'swallow': 550,\n",
       " 'insid': 551,\n",
       " 'consum': 552,\n",
       " 'everywher': 553,\n",
       " 'toward': 554,\n",
       " 'remind': 555,\n",
       " 'read': 556,\n",
       " 'toxic': 557,\n",
       " 'hous': 558,\n",
       " 'best': 559,\n",
       " 'cope': 560,\n",
       " 'etc': 561,\n",
       " 'rona': 562,\n",
       " 'hahahaha': 563,\n",
       " 'stay': 564,\n",
       " 'home': 565,\n",
       " 'forc': 566,\n",
       " 'brrrrrrrrr': 567,\n",
       " 'trap': 568,\n",
       " 'void': 569,\n",
       " 'dear': 570,\n",
       " 'whoever': 571,\n",
       " 'doubt': 572,\n",
       " 'fall': 573,\n",
       " 'criteria': 574,\n",
       " 'monoton': 575,\n",
       " 'task': 576,\n",
       " 'forward': 577,\n",
       " 'aspir': 578,\n",
       " 'whatsoev': 579,\n",
       " 'save': 580,\n",
       " 'grace': 581,\n",
       " 'felt': 582,\n",
       " 'maren': 583,\n",
       " 'idiot': 584,\n",
       " 'sound': 585,\n",
       " 'assur': 586,\n",
       " 'kept': 587,\n",
       " 'briefli': 588,\n",
       " 'worth': 589,\n",
       " 'okay': 590,\n",
       " 'open': 591,\n",
       " 'complet': 592,\n",
       " 'eas': 593,\n",
       " 'told': 594,\n",
       " 'els': 595,\n",
       " 'share': 596,\n",
       " 'intim': 597,\n",
       " 'dare': 598,\n",
       " 'devis': 599,\n",
       " 'crazi': 600,\n",
       " 'scenario': 601,\n",
       " 'us': 602,\n",
       " 'apart': 603,\n",
       " 'except': 604,\n",
       " 'ty': 605,\n",
       " 'problem': 606,\n",
       " 'face': 607,\n",
       " 'relat': 608,\n",
       " 'starr': 609,\n",
       " 'whenev': 610,\n",
       " 'better': 611,\n",
       " 'shine': 612,\n",
       " 'star': 613,\n",
       " 'pitch': 614,\n",
       " 'black': 615,\n",
       " 'sky': 616,\n",
       " 'none': 617,\n",
       " 'slowli': 618,\n",
       " 'drift': 619,\n",
       " 'exactli': 620,\n",
       " 'involv': 621,\n",
       " 'blame': 622,\n",
       " 'intent': 623,\n",
       " 'nonetheless': 624,\n",
       " 'clingi': 625,\n",
       " 'tendenc': 626,\n",
       " 'said': 627,\n",
       " 'exagger': 628,\n",
       " 'interact': 629,\n",
       " 'fair': 630,\n",
       " 'around': 631,\n",
       " 'extrem': 632,\n",
       " 'destruct': 633,\n",
       " 'spend': 634,\n",
       " 'four': 635,\n",
       " 'month': 636,\n",
       " 'legitim': 637,\n",
       " 'thank': 638,\n",
       " 'stress': 639,\n",
       " 'migrain': 640,\n",
       " 'far': 641,\n",
       " 'variou': 642,\n",
       " 'beyond': 643,\n",
       " 'control': 644,\n",
       " 'hold': 645,\n",
       " 'incred': 646,\n",
       " 'recept': 647,\n",
       " 'leav': 648,\n",
       " 'detail': 649,\n",
       " 'genuin': 650,\n",
       " 'concern': 651,\n",
       " 'dispel': 652,\n",
       " 'attract': 653,\n",
       " 'ten': 654,\n",
       " 'prior': 655,\n",
       " 'willingli': 656,\n",
       " 'pictur': 657,\n",
       " 'send': 658,\n",
       " 'snapchat': 659,\n",
       " 'hideou': 660,\n",
       " 'hindsight': 661,\n",
       " 'miss': 662,\n",
       " 'return': 663,\n",
       " 'fine': 664,\n",
       " 'high': 665,\n",
       " 'mainten': 666,\n",
       " 'altogeth': 667,\n",
       " 'sometim': 668,\n",
       " 'abruptli': 669,\n",
       " 'case': 670,\n",
       " 'morn': 671,\n",
       " 'whim': 672,\n",
       " 'signific': 673,\n",
       " 'also': 674,\n",
       " 'drive': 675,\n",
       " 'two': 676,\n",
       " 'laid': 677,\n",
       " 'stare': 678,\n",
       " 'ceil': 679,\n",
       " 'tear': 680,\n",
       " 'gone': 681,\n",
       " 'wrong': 682,\n",
       " 'hid': 683,\n",
       " 'outsid': 684,\n",
       " 'view': 685,\n",
       " 'contain': 686,\n",
       " 'luck': 687,\n",
       " 'reconnect': 688,\n",
       " 'idea': 689,\n",
       " 'onlin': 690,\n",
       " 'facebook': 691,\n",
       " 'lowest': 692,\n",
       " 'sent': 693,\n",
       " 'messag': 694,\n",
       " 'prepar': 695,\n",
       " 'wonder': 696,\n",
       " 'convers': 697,\n",
       " 'reminisc': 698,\n",
       " 'childhood': 699,\n",
       " 'rememb': 700,\n",
       " 'subject': 701,\n",
       " 'chang': 702,\n",
       " 'partli': 703,\n",
       " 'upset': 704,\n",
       " 'breakup': 705,\n",
       " 'especi': 706,\n",
       " 'decad': 707,\n",
       " 'joke': 708,\n",
       " 'red': 709,\n",
       " 'string': 710,\n",
       " 'fate': 711,\n",
       " 'dumb': 712,\n",
       " 'fact': 713,\n",
       " 'replic': 714,\n",
       " 'awkward': 715,\n",
       " 'freez': 716,\n",
       " 'bore': 717,\n",
       " 'subreddit': 718,\n",
       " 'rr': 719,\n",
       " 'kikpal': 720,\n",
       " 'friendship': 721,\n",
       " 'fight': 722,\n",
       " 'badger': 723,\n",
       " 'game': 724,\n",
       " 'throne': 725,\n",
       " 'boyfriend': 726,\n",
       " 'bottom': 727,\n",
       " 'fault': 728,\n",
       " 'dissolv': 729,\n",
       " 'join': 730,\n",
       " 'dissolut': 731,\n",
       " 'unlov': 732,\n",
       " 'true': 733,\n",
       " 'stem': 734,\n",
       " 'expand': 735,\n",
       " 'badli': 736,\n",
       " 'latch': 737,\n",
       " 'suffoc': 738,\n",
       " 'bother': 739,\n",
       " 'space': 740,\n",
       " 'weekend': 741,\n",
       " 'cancel': 742,\n",
       " 'plan': 743,\n",
       " 'favour': 744,\n",
       " 'sink': 745,\n",
       " 'mainli': 746,\n",
       " 'awak': 747,\n",
       " 'sustain': 748,\n",
       " 'offer': 749,\n",
       " 'must': 750,\n",
       " 'exhaust': 751,\n",
       " 'write': 752,\n",
       " 'recogn': 753,\n",
       " 'flaw': 754,\n",
       " 'knowledg': 755,\n",
       " 'entir': 756,\n",
       " 'zero': 757,\n",
       " 'notic': 758,\n",
       " 'fell': 759,\n",
       " 'earth': 760,\n",
       " 'fade': 761,\n",
       " 'oblivion': 762,\n",
       " 'steven': 763,\n",
       " 'galadriel': 764,\n",
       " 'monologu': 765,\n",
       " 'speak': 766,\n",
       " 'elvish': 767,\n",
       " 'amar': 768,\n",
       " 'prestar': 769,\n",
       " 'aen': 770,\n",
       " 'han': 771,\n",
       " 'matho': 772,\n",
       " 'ne': 773,\n",
       " 'nen': 774,\n",
       " 'water': 775,\n",
       " 'mathon': 776,\n",
       " 'ned': 777,\n",
       " 'cae': 778,\n",
       " 'noston': 779,\n",
       " 'gwilith': 780,\n",
       " 'smell': 781,\n",
       " 'air': 782,\n",
       " 'forg': 783,\n",
       " 'great': 784,\n",
       " 'ring': 785,\n",
       " 'three': 786,\n",
       " 'given': 787,\n",
       " 'elv': 788,\n",
       " 'immort': 789,\n",
       " 'wisest': 790,\n",
       " 'fairest': 791,\n",
       " 'be': 792,\n",
       " 'seven': 793,\n",
       " 'dwarf': 794,\n",
       " 'lord': 795,\n",
       " 'miner': 796,\n",
       " 'craftsmen': 797,\n",
       " 'mountain': 798,\n",
       " 'hall': 799,\n",
       " 'nine': 800,\n",
       " 'gift': 801,\n",
       " 'race': 802,\n",
       " 'men': 803,\n",
       " 'power': 804,\n",
       " 'within': 805,\n",
       " 'bound': 806,\n",
       " 'strength': 807,\n",
       " 'govern': 808,\n",
       " 'deceiv': 809,\n",
       " 'land': 810,\n",
       " 'mordor': 811,\n",
       " 'fire': 812,\n",
       " 'mount': 813,\n",
       " 'doom': 814,\n",
       " 'sauron': 815,\n",
       " 'master': 816,\n",
       " 'pour': 817,\n",
       " 'cruelti': 818,\n",
       " 'malic': 819,\n",
       " 'domin': 820,\n",
       " 'rule': 821,\n",
       " 'middl': 822,\n",
       " 'resist': 823,\n",
       " 'allianc': 824,\n",
       " 'march': 825,\n",
       " 'armi': 826,\n",
       " 'slope': 827,\n",
       " 'fought': 828,\n",
       " 'freedom': 829,\n",
       " 'victori': 830,\n",
       " 'near': 831,\n",
       " 'undon': 832,\n",
       " 'isildur': 833,\n",
       " 'son': 834,\n",
       " 'king': 835,\n",
       " 'sword': 836,\n",
       " 'enemi': 837,\n",
       " 'defeat': 838,\n",
       " 'chanc': 839,\n",
       " 'destroy': 840,\n",
       " 'evil': 841,\n",
       " 'forev': 842,\n",
       " 'heart': 843,\n",
       " 'easili': 844,\n",
       " 'corrupt': 845,\n",
       " 'betray': 846,\n",
       " 'forgotten': 847,\n",
       " 'becam': 848,\n",
       " 'legend': 849,\n",
       " 'myth': 850,\n",
       " 'half': 851,\n",
       " 'thousand': 852,\n",
       " 'came': 853,\n",
       " 'ensnar': 854,\n",
       " 'bearer': 855,\n",
       " 'creatur': 856,\n",
       " 'gollum': 857,\n",
       " 'tunnel': 858,\n",
       " 'misti': 859,\n",
       " 'gave': 860,\n",
       " 'unnatur': 861,\n",
       " 'five': 862,\n",
       " 'hundr': 863,\n",
       " 'poison': 864,\n",
       " 'mind': 865,\n",
       " 'gloom': 866,\n",
       " 'cave': 867,\n",
       " 'crept': 868,\n",
       " 'forest': 869,\n",
       " 'rumor': 870,\n",
       " 'grew': 871,\n",
       " 'shadow': 872,\n",
       " 'east': 873,\n",
       " 'whisper': 874,\n",
       " 'nameless': 875,\n",
       " 'perceiv': 876,\n",
       " 'abandon': 877,\n",
       " 'intend': 878,\n",
       " 'pick': 879,\n",
       " 'unlik': 880,\n",
       " 'hobbit': 881,\n",
       " 'bilbo': 882,\n",
       " 'baggin': 883,\n",
       " 'shire': 884,\n",
       " 'soon': 885,\n",
       " 'fortun': 886,\n",
       " 'sock': 887,\n",
       " 'heavi': 888,\n",
       " 'woke': 889,\n",
       " 'benefit': 890,\n",
       " 'accord': 891,\n",
       " 'healthlin': 892,\n",
       " 'prevent': 893,\n",
       " 'flash': 894,\n",
       " 'women': 895,\n",
       " 'cool': 896,\n",
       " 'core': 897,\n",
       " 'bodi': 898,\n",
       " 'temperatur': 899,\n",
       " 'improv': 900,\n",
       " 'crack': 901,\n",
       " 'heel': 902,\n",
       " 'cotton': 903,\n",
       " 'moistur': 904,\n",
       " 'dri': 905,\n",
       " 'increas': 906,\n",
       " 'potenti': 907,\n",
       " 'orgasm': 908,\n",
       " 'bbc': 909,\n",
       " 'research': 910,\n",
       " 'accident': 911,\n",
       " 'discov': 912,\n",
       " 'particip': 913,\n",
       " 'abil': 914,\n",
       " 'achiev': 915,\n",
       " 'percent': 916,\n",
       " 'decreas': 917,\n",
       " 'raynaud': 918,\n",
       " 'diseas': 919,\n",
       " 'area': 920,\n",
       " 'skin': 921,\n",
       " 'toe': 922,\n",
       " 'finger': 923,\n",
       " 'circul': 924,\n",
       " 'throb': 925,\n",
       " 'swell': 926,\n",
       " 'feet': 927,\n",
       " 'warm': 928,\n",
       " 'graveyard': 929,\n",
       " 'reddit': 930,\n",
       " 'eeri': 931,\n",
       " 'user': 932,\n",
       " 'often': 933,\n",
       " 'activ': 934,\n",
       " 'redditor': 935,\n",
       " 'dm': 936,\n",
       " 'liter': 937,\n",
       " 'anim': 938,\n",
       " 'seri': 939,\n",
       " 'flexibl': 940,\n",
       " 'santa': 941,\n",
       " 'clarita': 942,\n",
       " 'diet': 943,\n",
       " 'hollow': 944,\n",
       " 'shera': 945,\n",
       " 'extend': 946,\n",
       " 'repertoir': 947,\n",
       " 'text': 948,\n",
       " 'hour': 949,\n",
       " 'pretti': 950,\n",
       " 'anecdot': 951,\n",
       " 'late': 952,\n",
       " 'miser': 953,\n",
       " 'mine': 954,\n",
       " 'certain': 955,\n",
       " 'call': 956,\n",
       " 'chest': 957,\n",
       " 'nuisanc': 958,\n",
       " 'lol': 959,\n",
       " 'cring': 960,\n",
       " 'loath': 961,\n",
       " 'muster': 962,\n",
       " 'courag': 963,\n",
       " 'follow': 964,\n",
       " 'appreci': 965,\n",
       " 'let': 966,\n",
       " 'song': 967,\n",
       " 'lt': 968,\n",
       " 'vibe': 969,\n",
       " 'loverboy': 970,\n",
       " 'wall': 971,\n",
       " 'khai': 972,\n",
       " 'ur': 973,\n",
       " 'blackbear': 974,\n",
       " 'carwash': 975,\n",
       " 'stripteas': 976,\n",
       " 'slow': 977,\n",
       " 'listen': 978,\n",
       " 'effect': 979,\n",
       " 'easiest': 980,\n",
       " 'man': 981,\n",
       " 'drunk': 982,\n",
       " 'hodgkin': 983,\n",
       " 'lymphoma': 984,\n",
       " 'fam': 985,\n",
       " 'downward': 986,\n",
       " 'easi': 987,\n",
       " 'sweet': 988,\n",
       " 'arm': 989,\n",
       " 'next': 990,\n",
       " 'coward': 991,\n",
       " 'soul': 992,\n",
       " 'bullshit': 993,\n",
       " 'award': 994,\n",
       " 'expens': 995,\n",
       " 'emoji': 996,\n",
       " 'drown': 997,\n",
       " 'state': 998,\n",
       " 'univers': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary(data):\n",
    "    vocabulary = {PADDING: 0, UNKNOWN: 1}\n",
    "    for text in data:\n",
    "        for word in text:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "vocab = build_vocabulary(data)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,    3,    4, ...,    0,    0,    0],\n",
       "       [  53,   33,   54, ...,    0,    0,    0],\n",
       "       [  60,   61,   62, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [8043,  250,  328, ...,    0,    0,    0],\n",
       "       [ 296, 1296,  153, ...,    0,    0,    0],\n",
       "       [  41, 3646,  129, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 200\n",
    "def numericalize(data, vocabulary):\n",
    "    return [[vocabulary.get(word, vocabulary[UNKNOWN]) for word in text] for text in data]\n",
    "def truncate(data, length):\n",
    "    return [text[:length] if len(text) > length else text for text in data]\n",
    "def pad(data, length):\n",
    "    return [text[:length] + [0] * (length - len(text)) for text in data]\n",
    "\n",
    "numericalized = numericalize(data, vocab)\n",
    "truncated = truncate(numericalized, MAX_LENGTH)\n",
    "padded = pad(truncated, MAX_LENGTH)\n",
    "padded = np.array(padded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(modified_data, min_count=1, vector_size=150, window=5, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = Word2Vec.load(\"word2vec.model\")\n",
    "word_vectors = w2v.wv\n",
    "del model\n",
    "del w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x2c3e2cad190>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test wv\n",
    "word_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KimCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout=0.5):\n",
    "        super(KimCNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(word_vectors.vectors), freeze=True)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # for input of size (batch_size, sentence_length, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        # -> (batch_size, 1, sentence_length, embedding_dim)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        # -> [(batch_size, num_filters, sentence_length - fs + 1) for fs in filter_sizes]\n",
    "        x = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in x]\n",
    "        # -> [(batch_size, num_filters) for fs in filter_sizes]\n",
    "        x = torch.cat(x, 1)\n",
    "        # -> (batch_size, num_filters * len(filter_sizes))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, label, test_size=0.2, random_state=42)\n",
    "y_train = [0 if x == 'non-suicide' else 1 for x in y_train]\n",
    "y_test = [0 if x == 'non-suicide' else 1 for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, 64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94869\n",
      "94869\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vectors.vectors))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "KimCNN                                   --\n",
       "‚îú‚îÄEmbedding: 1-1                         (14,230,350)\n",
       "‚îú‚îÄModuleList: 1-2                        --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-1                       15,100\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-2                       30,100\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-3                       45,100\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-4                       60,100\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-5                       75,100\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-6                       90,100\n",
       "‚îú‚îÄDropout: 1-3                           --\n",
       "‚îú‚îÄLinear: 1-4                            1,202\n",
       "=================================================================\n",
       "Total params: 14,547,152\n",
       "Trainable params: 316,802\n",
       "Non-trainable params: 14,230,350\n",
       "================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = KimCNN(len(vocab), 150, 100, [1, 2, 3, 4, 5, 6], 2)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0052 |         Train Accuracy: 0.8607 |         Test Loss: 0.0038 |         Test Accuracy: 0.9055 |         Time taken: 34.4080\n",
      "Epoch 2: Train Loss: 0.0040 |         Train Accuracy: 0.8993 |         Test Loss: 0.0035 |         Test Accuracy: 0.9131 |         Time taken: 33.3091\n",
      "Epoch 3: Train Loss: 0.0036 |         Train Accuracy: 0.9093 |         Test Loss: 0.0033 |         Test Accuracy: 0.9178 |         Time taken: 33.4249\n",
      "Epoch 4: Train Loss: 0.0034 |         Train Accuracy: 0.9150 |         Test Loss: 0.0033 |         Test Accuracy: 0.9207 |         Time taken: 33.3016\n",
      "Epoch 5: Train Loss: 0.0032 |         Train Accuracy: 0.9201 |         Test Loss: 0.0033 |         Test Accuracy: 0.9215 |         Time taken: 33.3890\n",
      "Epoch 6: Train Loss: 0.0031 |         Train Accuracy: 0.9243 |         Test Loss: 0.0032 |         Test Accuracy: 0.9226 |         Time taken: 33.5029\n",
      "Epoch 7: Train Loss: 0.0030 |         Train Accuracy: 0.9270 |         Test Loss: 0.0031 |         Test Accuracy: 0.9236 |         Time taken: 33.4214\n",
      "Epoch 8: Train Loss: 0.0028 |         Train Accuracy: 0.9304 |         Test Loss: 0.0031 |         Test Accuracy: 0.9247 |         Time taken: 33.4698\n",
      "Epoch 9: Train Loss: 0.0027 |         Train Accuracy: 0.9333 |         Test Loss: 0.0031 |         Test Accuracy: 0.9256 |         Time taken: 33.4466\n",
      "Epoch 10: Train Loss: 0.0026 |         Train Accuracy: 0.9359 |         Test Loss: 0.0031 |         Test Accuracy: 0.9251 |         Time taken: 33.4532\n",
      "Epoch 11: Train Loss: 0.0025 |         Train Accuracy: 0.9389 |         Test Loss: 0.0031 |         Test Accuracy: 0.9258 |         Time taken: 33.5080\n",
      "Epoch 12: Train Loss: 0.0025 |         Train Accuracy: 0.9401 |         Test Loss: 0.0031 |         Test Accuracy: 0.9252 |         Time taken: 33.7509\n",
      "Epoch 13: Train Loss: 0.0024 |         Train Accuracy: 0.9425 |         Test Loss: 0.0031 |         Test Accuracy: 0.9252 |         Time taken: 33.9262\n",
      "Epoch 14: Train Loss: 0.0023 |         Train Accuracy: 0.9447 |         Test Loss: 0.0031 |         Test Accuracy: 0.9270 |         Time taken: 33.9112\n",
      "Epoch 15: Train Loss: 0.0022 |         Train Accuracy: 0.9462 |         Test Loss: 0.0032 |         Test Accuracy: 0.9260 |         Time taken: 34.1880\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_val_loss = 999\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    model.train() # Turn on train mode for gradient updates\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for data, target in train_dataloader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update metrics\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_accuracy += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    train_accuracy /= len(train_dataloader.dataset)\n",
    "    time_taken = time.time() - start_time\n",
    "\n",
    "    model.eval() # Turn on train mode to disable gradient updates\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            # Update metrics\n",
    "            test_loss += loss\n",
    "            pred = output.argmax(dim = 1, keepdim = True)\n",
    "            test_accuracy += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy /= len(test_dataloader.dataset)\n",
    "\n",
    "    # Log metrics\n",
    "    print(\n",
    "        f\"Epoch {epoch}: Train Loss: {train_loss:.4f} | \\\n",
    "        Train Accuracy: {train_accuracy:.4f} | \\\n",
    "        Test Loss: {test_loss:.4f} | \\\n",
    "        Test Accuracy: {test_accuracy:.4f} | \\\n",
    "        Time taken: {time_taken:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'kim_cnn.pth')\n",
    "torch.save(vocab, 'vocab.pth')\n",
    "torch.save(word_vectors, 'word_vectors.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
