{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "No_9ci7iEk4C"
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HqzGEMvPEk4F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Library to dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import opendatasets as od\n",
    "\n",
    "# Library tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Surpress warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Library sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pickle\n",
    "import pickle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ly5YTaNrEk4G",
    "outputId": "9a0d7fcf-2d0b-4ba2-ff17-0ad1e27d4b71",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv('clean_text.csv')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tI4zGvPDEk4H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data['clean_text'].astype(str)\n",
    "y = data['class']\n",
    "\n",
    "# encode class values as integers\n",
    "# Define label encoder\n",
    "encoder = LabelEncoder()\n",
    "# fit and transform\n",
    "encoded_y = encoder.fit_transform(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m7z7S5FdEk4H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test and train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,encoded_y, test_size=.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YksTB-KzEk4I"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sIEVlUS7Ek4I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 10_000\n",
    "max_length = 10_000\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the testing sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Convert the labels lists into numpy arrays\n",
    "training_labels = np.array(y_train)\n",
    "testing_labels = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word index dictionary\n",
    "np.save('word_index.npy', word_index)\n",
    "\n",
    "# Save Tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uw8lhhWEEk4J"
   },
   "source": [
    "# Basic Model Embeding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "of8FCYLUEk4J",
    "outputId": "c84ae2f6-9b35-4989-e7ae-80118688de8c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "# Initialize the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difine the callbacks\n",
    "# Save the best model\n",
    "checkpoint_callback = ModelCheckpoint(filepath=\"Basic Model Embeding Layers.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, verbose=1, mode=\"min\", min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "# Callbacks list\n",
    "callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt_f2RcWEk4J",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results_base = model.evaluate(testing_padded, testing_labels)\n",
    "\n",
    "# print results\n",
    "print(f'Test results - Loss: {results_base[0]} - Accuracy: {100*results_base[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tscfgTUxEk4K",
    "outputId": "3c42022d-dc3a-49a8-d2e8-762226ac2682",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "# Plot the accuracy and loss\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B0Msocs1Ek4K"
   },
   "source": [
    "# Model Basic + Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smU8ZKgyEk4K",
    "outputId": "4eaa90c3-6807-4370-f2f5-25139bbb2192",
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "# Initialize the model\n",
    "model_GlobalAveragePooling1D = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_GlobalAveragePooling1D.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_GlobalAveragePooling1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difine the callbacks\n",
    "# Save the best model\n",
    "checkpoint_callback = ModelCheckpoint(filepath=\"Model Basic + Global Average Pooling.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, verbose=1, mode=\"min\", min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "# Callbacks list\n",
    "callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgUSXmCqEk4L",
    "outputId": "ba0c3d0c-5aa0-4624-b5f8-ffef531433e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "history_GlobalAveragePooling1D = model_GlobalAveragePooling1D.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results_GlobalAveragePooling1D = model_GlobalAveragePooling1D.evaluate(testing_padded, testing_labels)\n",
    "\n",
    "# print results\n",
    "print(f'Test results - Loss: {results_GlobalAveragePooling1D[0]} - Accuracy: {100*results_GlobalAveragePooling1D[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lb9p_i4Ek4L",
    "outputId": "d9ee52e8-a47b-492e-a4f7-80afa24eed24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss\n",
    "plot_graphs(history_GlobalAveragePooling1D, \"accuracy\")\n",
    "plot_graphs(history_GlobalAveragePooling1D, \"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sffDRy1uEk4N"
   },
   "source": [
    "# Model Embedding + Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbpy4arMEk4N"
   },
   "outputs": [],
   "source": [
    "kernel_size = 5\n",
    "filters = 128\n",
    "\n",
    "# Initialize the model\n",
    "model_Conv1D = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_Conv1D.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_Conv1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difine the callbacks\n",
    "# Save the best model\n",
    "checkpoint_callback = ModelCheckpoint(filepath=\"Model Embedding + Conv1D.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, verbose=1, mode=\"min\", min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "# Callbacks list\n",
    "callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNv2Rk3bEk4N"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "history_Conv1D = model_Conv1D.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results_Conv1D = model_Conv1D.evaluate(testing_padded, testing_labels)\n",
    "\n",
    "# print results\n",
    "print(f'Test results - Loss: {results_Conv1D[0]} - Accuracy: {100*results_Conv1D[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu8IIFrnEk4N"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss\n",
    "plot_graphs(history_Conv1D, \"accuracy\")\n",
    "plot_graphs(history_Conv1D, \"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model\n",
    "model_base = tf.keras.models.load_model('Basic-Model-Embeding-Layers.h5')\n",
    "model_GlobalAveragePooling1D = tf.keras.models.load_model('Model-Basic-Global-Average-Pooling.h5')\n",
    "model_Conv1D = tf.keras.models.load_model('Model-Embedding-Conv1D.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tweet\n",
    "twt = ['I will kill myself']\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "twt = pad_sequences(twt, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Predict the sentiment\n",
    "prediction_base = model_base.predict(twt)\n",
    "\n",
    "# Print the prediction\n",
    "if(np.argmax(prediction_base) == 0):\n",
    "    print(\"Potential Suicide Post\")\n",
    "elif (np.argmax(prediction_base) == 1):\n",
    "    print(\"Non Suicide Post\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Basic + Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Basic + Global Average Pooling\n",
    "prediction_Globalave = model_GlobalAveragePooling1D.predict(twt)\n",
    "\n",
    "# Print the prediction\n",
    "if(np.argmax(prediction_Globalave) == 0):\n",
    "    print(\"Potential Suicide Post\")\n",
    "elif (np.argmax(prediction_Globalave) == 1):\n",
    "    print(\"Non Suicide Post\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Embeding + Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Embeding + Conv1D\n",
    "prediction_EmbedCov1D = model_Conv1D.predict(twt)\n",
    "\n",
    "# Print the prediction\n",
    "if(np.argmax(prediction_EmbedCov1D) == 0):\n",
    "    print(\"Potential Suicide Post\")\n",
    "elif (np.argmax(prediction_EmbedCov1D) == 1):\n",
    "    print(\"Non Suicide Post\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model\n",
    "results=pd.DataFrame({'Model':['Base Model','Base + Global Average Pooling','Base + CNN'],\n",
    "                     'Accuracy Score':[results_base[1],results_GlobalAveragePooling1D[1],results_Conv1D[1]]})\n",
    "result_df=results.sort_values(by='Accuracy Score', ascending=False)\n",
    "result_df=result_df.set_index('Model')\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
