{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\84359\\AppData\\Local\\Temp\\ipykernel_2308\\2218911435.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vocab = torch.load('vocab.pth')\n",
      "C:\\Users\\84359\\AppData\\Local\\Temp\\ipykernel_2308\\2218911435.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  word_vectors = torch.load('word_vectors.pth')\n"
     ]
    }
   ],
   "source": [
    "vocab = torch.load('vocab.pth')\n",
    "word_vectors = torch.load('word_vectors.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KimCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout=0.5):\n",
    "        super(KimCNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(word_vectors.vectors), freeze=False)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # for input of size (batch_size, sentence_length, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        # -> (batch_size, 1, sentence_length, embedding_dim)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        # -> [(batch_size, num_filters, sentence_length - fs + 1) for fs in filter_sizes]\n",
    "        x = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in x]\n",
    "        # -> [(batch_size, num_filters) for fs in filter_sizes]\n",
    "        x = torch.cat(x, 1)\n",
    "        # -> (batch_size, num_filters * len(filter_sizes))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\84359\\AppData\\Local\\Temp\\ipykernel_2308\\2444214139.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('kim_cnn.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KimCNN(len(vocab), 150, 100, [1, 2, 3, 4, 5, 6], 2)\n",
    "model.load_state_dict(torch.load('kim_cnn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\84359\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\84359\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\84359\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')\n",
    "def preprocess(txt):\n",
    "    # Remove URL\n",
    "    txt = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))','', txt)\n",
    "    # Also remove hashtags since in reddit they are a kind of link\n",
    "    txt = re.sub(r'#([^\\s]+)', '', txt)\n",
    "    # Remove all the special characters\n",
    "    txt = re.sub(r'\\W', ' ', txt)\n",
    "    # Remove numeric\n",
    "    txt = re.sub(r\"\\d+\", \"\", txt)\n",
    "    # There are words that are glueTogetherSinceRedditCommentFormattingIsWeird\n",
    "    match = re.search(r'[a-z][A-Z]', txt)\n",
    "    if match:\n",
    "        idx = match.start()\n",
    "        txt = txt[:idx+1]+' '+txt[idx+1:]\n",
    "    # remove all single characters\n",
    "    txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n",
    "    # Remove single characters from the start\n",
    "    txt = re.sub(r'\\^[a-zA-Z]\\s+', ' ', txt) \n",
    "    # Substituting multiple spaces with single space\n",
    "    txt = re.sub(r'\\s+', ' ', txt, flags=re.I)\n",
    "    # Converting to Lowercase\n",
    "    txt = txt.lower()\n",
    "    # Remove whitespace at begin and end string\n",
    "    txt = txt.strip()\n",
    "\n",
    "    txt = txt.split()\n",
    "    txt = [word for word in txt if word not in stopword]\n",
    "    txt = ' '.join(txt)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    txt = txt.split()\n",
    "    txt = [stemmer.stem(word) for word in txt]\n",
    "    txt = ' '.join(txt)\n",
    "\n",
    "    print(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = '<PAD>'\n",
    "UNKNOWN = '<BRUH>'\n",
    "MAX_LEN = 200\n",
    "def prepare_data(sentence):\n",
    "    sentence = preprocess(sentence)\n",
    "    sentence = word_tokenize(sentence)\n",
    "    res = []\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            res.append(vocab[UNKNOWN])\n",
    "        else:\n",
    "            res.append(vocab[word])\n",
    "    if len(res) > MAX_LEN:\n",
    "        res = res[:MAX_LEN]\n",
    "    else:\n",
    "        res += [vocab[PADDING]]*(MAX_LEN-len(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = \"\"\"\n",
    "i hit myself in the balls accidentally\n",
    "Gaming\n",
    "ouch\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "another_sentence = \"\"\"\n",
    "How to deal with the utter meaningless of it all?\n",
    "Brain damage at a young age has left me with the inability to socialize. I hate my situation, i hate myself for my inability to change. I hate myself for my utter stupidity when it comes to social awareness.\n",
    "\n",
    "The lack of meaningful relationships extending my entire life has left me dead inside. Been this way for a long time. What point is there in continuing to suffer? It never gets better. I'll continue to rot in my room for the remainder of my miserable life if I don't make a change, but change seems impossible for someone like me. And even if by some miracle I do change, would it even make me happy? I know, at the end of the day, nothing matters.\n",
    "\n",
    "Suicide is a permanent solution to a temporary problem. Life is a temporary problem, sure. But my problems are permanent. Why suffer any longer? My life has zero value\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tire peopl point much eat titl say hate much yesterday ate one serv someth made pasta casserol littl later ate two serv hungri dad made comment ask mani serv eaten call fat sinc child even though underweight peopl get comment famili suck wish would stop\n",
      "cook good sir burn fri deep fri grill boil roast fri cook\n"
     ]
    }
   ],
   "source": [
    "# List of sentences\n",
    "sentences = [\n",
    "    \"\"\"I’m tired of people pointing out how much I eat\n",
    "Title says it all.\n",
    "\n",
    "I hate it so much. Yesterday I ate one serving of something I had made (pasta casserole). A little while later, I ate two more servings because I was hungry again. My dad made a comment about it, asking me how many servings I had eaten.\n",
    "\n",
    "I’ve been called fat since I was a child (even though I was underweight) by other people, and now I’m getting comments by my family.\n",
    "\n",
    "It just sucks, and I wish it would stop.\n",
    "\"\"\",\n",
    "\"\"\"You are not just cooked.\n",
    "You good sir are burned, fried, then deep-fried, grilled, boiled, roasted, fried again and then cooked\"\"\"\n",
    "]\n",
    "\n",
    "# Corresponding labels: 1 for suicidal, 0 for non-suicidal\n",
    "sentences = [prepare_data(x) for x in sentences]\n",
    "\n",
    "sentences = torch.tensor(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(sentences)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
